import os
from dotenv import load_dotenv
import openai

load_dotenv()

# Agent to determine the data source of the query
class LLMAsJudge:
    def __init__(self):
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.client = openai.OpenAI(api_key=self.openai_key)

    def judge_summary(self, query, original_chunk, summarized_chunk):
        prompt = f"""
        Evaluate the correctness of the summarized chunk based on how well it accurately summarizes the original chunk and answers the question.

        Rate the summary on a scale of 1 to 5, where:
        - 1: Very inaccurate summary that does not address the question.
        - 5: Very accurate and thorough summary that fully answers the question.

        ## Question:
        {query}

        ## Original Chunk:
        {original_chunk}

        ## Generated Answer:
        {summarized_chunk}

        ## Score: [Provide an integer score between 1 and 5 with no additional text] 
        """

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "developer", 
                    "content": "You are a judge assigned to evaluate the summary generated by an LLM Agent."},
                    {"role": "user", 
                     "content": prompt}]
        )
        return response.choices[0].message.content
    
    def judge_answer(self, query, reference_answer, generated_answer):
        prompt = f"""
        Evaluate the correctness of the generated answer to the following question, comparing it to the reference answer. 
        Rate them on a scale of 1-5, where 1 indicates a very incorrect response and 5 indicates a very accurate and thorough response.

        ## Question:
        {query}

        ## Reference Answer:
        {reference_answer}

        ## Generated Answer:
        {generated_answer}

        ## Score: [The score should be an integer between 1 and 5, without any additional text]
        """

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "developer", 
                    "content": "You are a judge assigned to evaluate the response generated by a Retrieval-Augmented Generation (RAG) powered Large Language Model (LLM) chatbot. The task involves assessing the modelâ€™s answer to a query related to financial regulations and compliance."},
                    {"role": "user", 
                     "content": prompt}]
        )
        return response.choices[0].message.content

if __name__ == "__main__":

    judge = LLMAsJudge()
    query = 'What is a bank'
    reference_answer = 'Bank is any company that holds a valid bank licence'
    generated_answer = 'A bank is defined as a financial institution that is licensed to accept deposits from the public, provide loans, and offer various financial services (Banking Act 1970, 48A). It operates within a regulatory framework overseen by the Authority, which includes monitoring its management and compliance with legal obligations (Banking Act 1970, 48A). Additionally, a bank may be subject to interventions by the Authority in cases of insolvency or failure to meet obligations (Banking Act 1970, 48A). The term also encompasses the business affairs and property pertaining to the bank (Banking Act 1970, 48A). Furthermore, a bank in Singapore is authorized to engage in banking activities such as accepting deposits and extending loans (Banking Act 1970, 40A).'
    score = judge.get_llm_score(query, reference_answer, generated_answer)
    print('LLM As Judge Score:', score)